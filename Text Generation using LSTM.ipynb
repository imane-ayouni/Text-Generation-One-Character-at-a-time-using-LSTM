{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecaabf2",
   "metadata": {},
   "source": [
    "# Text Generation using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e410245",
   "metadata": {},
   "source": [
    "## Topic\n",
    "\n",
    "In this notebook I will be experimenting with an LSTM to generate text. I will be using a model text (Sherlock Holmes story) that I will train my network on to finally generate new text one charachter at a time. To do so, I will process my text first by using onehot encoding, then I will create the network and train it, finally I will generate a new charachter based on the last charachter generated by the model. So let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ff657",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Generate new text one charachter at a time using an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558013f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58870c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22539ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\imane\\Downloads\\archive (23)\\text.txt', \"r\", encoding='utf-8') as f:\n",
    "    text= f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "855f6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[1255:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0fc56a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' IN BOHEMIA\\n\\n\\nI.\\n\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard him\\nmention her under any other name. In his eyes she eclipses and\\npredominates the whole of her sex. It was not tha'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4defc",
   "metadata": {},
   "source": [
    "So first of all I imported the text and deleted the intro to the story (Glossary and such), then I printed a sample paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd99c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "662ce2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = [\"&\", \"\\n\", \"$\", \"@\", '\"\"', \"â€”\",\"(\", \"_\" ]\n",
    "for char in special_chars:\n",
    "    text = text.replace(char, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9341a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' IN BOHEMIA   I.  To Sherlock Holmes she is always  the  woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not tha'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287f70d",
   "metadata": {},
   "source": [
    "Next I removed the special charachters to keep only the row text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8222511",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eca645cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 12, 14, 50, 15,  9, 23, 30, 34, 12,  7, 50, 50, 50, 12, 43, 50,\n",
       "       50, 32, 24, 50,  0, 16,  3, 38,  1, 24, 10, 56, 50, 23, 24,  1, 53,\n",
       "        3, 47, 50, 47, 16,  3, 50, 31, 47, 50,  8,  1, 54,  8, 26, 47, 50,\n",
       "       50, 11, 16,  3, 50, 50, 54, 24, 53,  8, 20, 43, 50, 12, 50, 16,  8,\n",
       "        2,  3, 50, 47,  3,  1, 33, 24, 53, 50, 16,  3,  8, 38, 33, 50, 16,\n",
       "       31, 53, 50, 53,  3, 20, 11, 31, 24, 20, 50, 16,  3, 38, 50])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd28e3b",
   "metadata": {},
   "source": [
    "In the above I encoded the characters by giving them each a unique number after of course splitting them, then I created two dictinaries one that goves the unique number of each character and the other that gives a the character associated to each number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d79d8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6fde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make \n",
    "    char_per_batch = batch_size * seq_length\n",
    "    n_batches = len(arr)//char_per_batch\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:char_per_batch*n_batches]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] =  x[:, 1:] , arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460b60a",
   "metadata": {},
   "source": [
    "The function get batches takes in the encoded characters and returns batches of them where the x is a sequence of numbers and the y is the same sequence shifted by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3390ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e1221f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, n_hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cd970",
   "metadata": {},
   "source": [
    "The class CharRNN is where I created my model, it contains a stacked lstm layer, a dropout layer and a fully connexted layers with a hidden and cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b69db678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss)\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.average(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1e3e4",
   "metadata": {},
   "source": [
    "The training function specifies a loss criterion and optimizer first, then it iterates through the batches, passing every input through the network and calculating the loss based on the output and actual label. Then the validation step goes through the same process to produce a validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63bb1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(67, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden= 512\n",
    "n_layers= 2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "82d03b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/200... Step: 10... Loss: 1.7791... Val Loss: nan\n",
      "Epoch: 7/200... Step: 20... Loss: 1.7315... Val Loss: nan\n",
      "Epoch: 10/200... Step: 30... Loss: 1.7415... Val Loss: nan\n",
      "Epoch: 14/200... Step: 40... Loss: 1.6750... Val Loss: nan\n",
      "Epoch: 17/200... Step: 50... Loss: 1.6495... Val Loss: nan\n",
      "Epoch: 20/200... Step: 60... Loss: 1.6609... Val Loss: nan\n",
      "Epoch: 24/200... Step: 70... Loss: 1.6082... Val Loss: nan\n",
      "Epoch: 27/200... Step: 80... Loss: 1.5694... Val Loss: nan\n",
      "Epoch: 30/200... Step: 90... Loss: 1.5902... Val Loss: nan\n",
      "Epoch: 34/200... Step: 100... Loss: 1.5362... Val Loss: nan\n",
      "Epoch: 37/200... Step: 110... Loss: 1.4924... Val Loss: nan\n",
      "Epoch: 40/200... Step: 120... Loss: 1.5085... Val Loss: nan\n",
      "Epoch: 44/200... Step: 130... Loss: 1.4582... Val Loss: nan\n",
      "Epoch: 47/200... Step: 140... Loss: 1.4157... Val Loss: nan\n",
      "Epoch: 50/200... Step: 150... Loss: 1.4152... Val Loss: nan\n",
      "Epoch: 54/200... Step: 160... Loss: 1.3540... Val Loss: nan\n",
      "Epoch: 57/200... Step: 170... Loss: 1.3153... Val Loss: nan\n",
      "Epoch: 60/200... Step: 180... Loss: 1.3330... Val Loss: nan\n",
      "Epoch: 64/200... Step: 190... Loss: 1.2910... Val Loss: nan\n",
      "Epoch: 67/200... Step: 200... Loss: 1.2359... Val Loss: nan\n",
      "Epoch: 70/200... Step: 210... Loss: 1.2389... Val Loss: nan\n",
      "Epoch: 74/200... Step: 220... Loss: 1.1954... Val Loss: nan\n",
      "Epoch: 77/200... Step: 230... Loss: 1.1660... Val Loss: nan\n",
      "Epoch: 80/200... Step: 240... Loss: 1.1479... Val Loss: nan\n",
      "Epoch: 84/200... Step: 250... Loss: 1.0763... Val Loss: nan\n",
      "Epoch: 87/200... Step: 260... Loss: 1.0480... Val Loss: nan\n",
      "Epoch: 90/200... Step: 270... Loss: 1.0421... Val Loss: nan\n",
      "Epoch: 94/200... Step: 280... Loss: 1.0105... Val Loss: nan\n",
      "Epoch: 97/200... Step: 290... Loss: 0.9725... Val Loss: nan\n",
      "Epoch: 100/200... Step: 300... Loss: 0.9663... Val Loss: nan\n",
      "Epoch: 104/200... Step: 310... Loss: 0.9346... Val Loss: nan\n",
      "Epoch: 107/200... Step: 320... Loss: 0.8825... Val Loss: nan\n",
      "Epoch: 110/200... Step: 330... Loss: 0.8673... Val Loss: nan\n",
      "Epoch: 114/200... Step: 340... Loss: 0.8243... Val Loss: nan\n",
      "Epoch: 117/200... Step: 350... Loss: 0.7925... Val Loss: nan\n",
      "Epoch: 120/200... Step: 360... Loss: 0.7860... Val Loss: nan\n",
      "Epoch: 124/200... Step: 370... Loss: 0.7434... Val Loss: nan\n",
      "Epoch: 127/200... Step: 380... Loss: 0.7163... Val Loss: nan\n",
      "Epoch: 130/200... Step: 390... Loss: 0.7212... Val Loss: nan\n",
      "Epoch: 134/200... Step: 400... Loss: 0.6802... Val Loss: nan\n",
      "Epoch: 137/200... Step: 410... Loss: 0.6481... Val Loss: nan\n",
      "Epoch: 140/200... Step: 420... Loss: 0.6325... Val Loss: nan\n",
      "Epoch: 144/200... Step: 430... Loss: 0.6221... Val Loss: nan\n",
      "Epoch: 147/200... Step: 440... Loss: 0.5850... Val Loss: nan\n",
      "Epoch: 150/200... Step: 450... Loss: 0.5859... Val Loss: nan\n",
      "Epoch: 154/200... Step: 460... Loss: 0.5528... Val Loss: nan\n",
      "Epoch: 157/200... Step: 470... Loss: 0.5331... Val Loss: nan\n",
      "Epoch: 160/200... Step: 480... Loss: 0.5196... Val Loss: nan\n",
      "Epoch: 164/200... Step: 490... Loss: 0.4942... Val Loss: nan\n",
      "Epoch: 167/200... Step: 500... Loss: 0.4803... Val Loss: nan\n",
      "Epoch: 170/200... Step: 510... Loss: 0.4717... Val Loss: nan\n",
      "Epoch: 174/200... Step: 520... Loss: 0.4490... Val Loss: nan\n",
      "Epoch: 177/200... Step: 530... Loss: 0.4240... Val Loss: nan\n",
      "Epoch: 180/200... Step: 540... Loss: 0.4155... Val Loss: nan\n",
      "Epoch: 184/200... Step: 550... Loss: 0.4069... Val Loss: nan\n",
      "Epoch: 187/200... Step: 560... Loss: 0.3871... Val Loss: nan\n",
      "Epoch: 190/200... Step: 570... Loss: 0.3889... Val Loss: nan\n",
      "Epoch: 194/200... Step: 580... Loss: 0.3705... Val Loss: nan\n",
      "Epoch: 197/200... Step: 590... Loss: 0.3620... Val Loss: nan\n",
      "Epoch: 200/200... Step: 600... Loss: 0.3555... Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 200\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4f20f",
   "metadata": {},
   "source": [
    "The training happened over 200 epochs, through which the training loss decreses to reach .35, unfortunately I cannot see the testing loss (I've tried to solve the issue but couldn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a4ae28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_200_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455957f8",
   "metadata": {},
   "source": [
    "Then I saved the latest model with all its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "69ff7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141762d",
   "metadata": {},
   "source": [
    "The predict function takes in a character and predicts the next character by randomly picking through the topk characters i.e the characters with the highest probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05718c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fa1a1",
   "metadata": {},
   "source": [
    "Then the sample function starts off with a prime word, takes as input a character and produces the next one to be also used as input in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1615597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellonoro ale therered he alalerorare harale he hed are as halarerarere the th there th heras he as t as t h thare arerere h halalinono there therored has arare hale thas thare aro has t thared th thered as thaline tharerererered as are as he arore arone as thalin hererereras he thararare the thed the are t he therered hererero are aleraro hereronerererase t thale he arerase there ale to ton hereredo halaroras thas aredo t hare thalarere t thare he harerored alito are t thed hed arored the torored tore ase h therereras halis. hale the h th aras t arero has the t has arederere t the harerered aronore he alin alarasedoras tharo there h aralas harerero has the thaled torered arerorere h th haredo the heredere as alithared arerarere arererede haras he the h he t aled alereroras aliserero here hale thalas heded thered haras he thas arered torerarered hare tharererered thered toreras herere th t alaro alared hed are he h as alerororered t to to aled alithared tharerere to ale has ared asere aro h \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Hello', top_k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab6d581a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('lstm_200_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d576f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Sherlock said haro the has alinerono hareras ale alarerored areded as h h as arerereras arered alas thas he as alin hed he arererere he thareras h to are t he tharered haleron t tharonerere the halithale here here herared there to h ale tono t hale tharere hero arerere he as therere torerere halin halere to hed he harerererered torored alalerere he alithed th he as are t to aro to ale here hed has hed aserered the as the he hed he are to harerere he therasererererererorerere as the he he as t are thas. as h hare thed as heded as asere he thare here arereredore alared aleralerare hare hed therero aras ton aro he th hed thared are therere has haled h heralin as he the h ared the thale the as hede the torere tore has thered hererere as he h arerarero the th here herere ared thed hase aron thalis heralered harerere h the hed areraserere tharororarede he halere asedon hedorere thede herore t harono as arere ased thererere thed thalasered the as the as hed are herereras t hered as. thed hed ton there th heralalede herored he as aras as herere tono herore the alas hed thas t th hed hero as. there he aron h arere to as thedo he thalered th harere alinere here to ale aroned hale harederas are are herere th as. heron hererere harere th as ale as arere there there as to therered he he thed alaledo as hed are has thed t herere he thererededereras hararore th the harere he there thas has as tone tharo toredered t he th has aserere has thed t th aredonon thas th he herered as aras to as h thered t there herare thas t thas arereded thedored theras arere hare to herere the thalasererered hereras t harero thare to he therererereralis thas arererarale there to thalas herorere halino has tore as. th here he t thererored he here hare he hed t tharo thalaline the thas he alase alarerere thalarare the alerorere arerore as he to ale hale herare halererere alarale th are has theredererere as ased th heredo hede has thas hed th therono tharererore aro thalalis here alereronerere ared has h to thas thed a\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, top_k=3, prime=\"And Sherlock said\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967447e",
   "metadata": {},
   "source": [
    "Finally I tried to generate some text, the results are not very good seen as the text generated doesn't make much sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3b0a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
