{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecaabf2",
   "metadata": {},
   "source": [
    "# Text Generation using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e410245",
   "metadata": {},
   "source": [
    "## Topic\n",
    "\n",
    "In this notebook I will be experimenting with an LSTM to generate text. I will be using a model text (Sherlock Holmes story) that I will train my network on to finally generate new text one charachter at a time. To do so, I will process my text first by using embedding, then I will create the network and train it, finally I will generate a new charachter based on the last charachter generated by the model. So let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ff657",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Generate new text one charachter at a time using an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558013f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "58870c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "22539ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\imane\\Downloads\\archive (23)\\text.txt', \"r\", encoding='utf-8') as f:\n",
    "    text= f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "855f6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[1255:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a0fc56a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' IN BOHEMIA\\n\\n\\nI.\\n\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard him\\nmention her under any other name. In his eyes she eclipses and\\npredominates the whole of her sex. It was not tha'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4defc",
   "metadata": {},
   "source": [
    "So first of all I imported the text and deleted the intro to the story (Glossary and such), then I printed a sample paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dd99c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "662ce2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = [\"&\", \"\\n\", \"$\", \"@\", '\"\"', \"—\",\"(\", \"_\" ]\n",
    "for char in special_chars:\n",
    "    text = text.replace(char, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b9341a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' IN BOHEMIA   I.  To Sherlock Holmes she is always  the  woman. I have seldom heard him mention her under any other name. In his eyes she eclipses and predominates the whole of her sex. It was not tha'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287f70d",
   "metadata": {},
   "source": [
    "Next I removed the special charachters to keep only the row text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f8222511",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "eca645cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 12, 14, 50, 15,  9, 23, 30, 34, 12,  7, 50, 50, 50, 12, 43, 50,\n",
       "       50, 32, 24, 50,  0, 16,  3, 38,  1, 24, 10, 56, 50, 23, 24,  1, 53,\n",
       "        3, 47, 50, 47, 16,  3, 50, 31, 47, 50,  8,  1, 54,  8, 26, 47, 50,\n",
       "       50, 11, 16,  3, 50, 50, 54, 24, 53,  8, 20, 43, 50, 12, 50, 16,  8,\n",
       "        2,  3, 50, 47,  3,  1, 33, 24, 53, 50, 16,  3,  8, 38, 33, 50, 16,\n",
       "       31, 53, 50, 53,  3, 20, 11, 31, 24, 20, 50, 16,  3, 38, 50])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd28e3b",
   "metadata": {},
   "source": [
    "In the above I encoded the characters by giving them each a unique number after of course splitting them, then I created two dictinaries one that goves the unique number of each character and the other that gives a the character associated to each number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d79d8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f6fde53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    ## TODO: Get the number of batches we can make \n",
    "    char_per_batch = batch_size * seq_length\n",
    "    n_batches = len(arr)//char_per_batch\n",
    "    \n",
    "    ## TODO: Keep only enough characters to make full batches\n",
    "    arr = arr[:char_per_batch*n_batches]\n",
    "    \n",
    "    ## TODO: Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "    \n",
    "    ## TODO: Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] =  x[:, 1:] , arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460b60a",
   "metadata": {},
   "source": [
    "The function get batches takes in the encoded characters and returns batches of them where the x is a sequence of numbers and the y is the same sequence shifted by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3390ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3e1221f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, embedding_dim = 200, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the layers of the model\n",
    "        self.embedding = nn.Embedding(len(tokens), embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        x = x.to(torch.long)\n",
    "        em = self.embedding(x)\n",
    "        #em = em.squeeze()\n",
    "        #hidden =  tuple([each.squeeze() for each in hidden])\n",
    "        r_output, hidden = self.lstm(em, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cd970",
   "metadata": {},
   "source": [
    "The class CharRNN is where I created my model, it contains a stacked lstm layer, a dropout layer and a fully connexted layers with a hidden and cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "b69db678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            #x = one_hot_encode(x, n_chars)\n",
    "            x,y  = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(x, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, y.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    #x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss)\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.average(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1e3e4",
   "metadata": {},
   "source": [
    "The training function specifies a loss criterion and optimizer first, then it iterates through the batches, passing every input through the network and calculating the loss based on the output and actual label. Then the validation step goes through the same process to produce a validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "63bb1789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (embedding): Embedding(67, 50)\n",
      "  (lstm): LSTM(50, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden= 512\n",
    "n_layers= 2\n",
    "embedding_dim = 50\n",
    "net = CharRNN(chars, embedding_dim, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "82d03b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/200... Step: 10... Loss: 3.1573... Val Loss: nan\n",
      "Epoch: 7/200... Step: 20... Loss: 3.0714... Val Loss: nan\n",
      "Epoch: 10/200... Step: 30... Loss: 3.0035... Val Loss: nan\n",
      "Epoch: 14/200... Step: 40... Loss: 2.8458... Val Loss: nan\n",
      "Epoch: 17/200... Step: 50... Loss: 2.6550... Val Loss: nan\n",
      "Epoch: 20/200... Step: 60... Loss: 2.5293... Val Loss: nan\n",
      "Epoch: 24/200... Step: 70... Loss: 2.4049... Val Loss: nan\n",
      "Epoch: 27/200... Step: 80... Loss: 2.2995... Val Loss: nan\n",
      "Epoch: 30/200... Step: 90... Loss: 2.2532... Val Loss: nan\n",
      "Epoch: 34/200... Step: 100... Loss: 2.1746... Val Loss: nan\n",
      "Epoch: 37/200... Step: 110... Loss: 2.0948... Val Loss: nan\n",
      "Epoch: 40/200... Step: 120... Loss: 2.0720... Val Loss: nan\n",
      "Epoch: 44/200... Step: 130... Loss: 2.0132... Val Loss: nan\n",
      "Epoch: 47/200... Step: 140... Loss: 1.9385... Val Loss: nan\n",
      "Epoch: 50/200... Step: 150... Loss: 1.9311... Val Loss: nan\n",
      "Epoch: 54/200... Step: 160... Loss: 1.8753... Val Loss: nan\n",
      "Epoch: 57/200... Step: 170... Loss: 1.8233... Val Loss: nan\n",
      "Epoch: 60/200... Step: 180... Loss: 1.8107... Val Loss: nan\n",
      "Epoch: 64/200... Step: 190... Loss: 1.7412... Val Loss: nan\n",
      "Epoch: 67/200... Step: 200... Loss: 1.6758... Val Loss: nan\n",
      "Epoch: 70/200... Step: 210... Loss: 1.6718... Val Loss: nan\n",
      "Epoch: 74/200... Step: 220... Loss: 1.6216... Val Loss: nan\n",
      "Epoch: 77/200... Step: 230... Loss: 1.5489... Val Loss: nan\n",
      "Epoch: 80/200... Step: 240... Loss: 1.5386... Val Loss: nan\n",
      "Epoch: 84/200... Step: 250... Loss: 1.4966... Val Loss: nan\n",
      "Epoch: 87/200... Step: 260... Loss: 1.4278... Val Loss: nan\n",
      "Epoch: 90/200... Step: 270... Loss: 1.4017... Val Loss: nan\n",
      "Epoch: 94/200... Step: 280... Loss: 1.3585... Val Loss: nan\n",
      "Epoch: 97/200... Step: 290... Loss: 1.2942... Val Loss: nan\n",
      "Epoch: 100/200... Step: 300... Loss: 1.2892... Val Loss: nan\n",
      "Epoch: 104/200... Step: 310... Loss: 1.2491... Val Loss: nan\n",
      "Epoch: 107/200... Step: 320... Loss: 1.1683... Val Loss: nan\n",
      "Epoch: 110/200... Step: 330... Loss: 1.1293... Val Loss: nan\n",
      "Epoch: 114/200... Step: 340... Loss: 1.1036... Val Loss: nan\n",
      "Epoch: 117/200... Step: 350... Loss: 1.0298... Val Loss: nan\n",
      "Epoch: 120/200... Step: 360... Loss: 1.0027... Val Loss: nan\n",
      "Epoch: 124/200... Step: 370... Loss: 0.9977... Val Loss: nan\n",
      "Epoch: 127/200... Step: 380... Loss: 0.9227... Val Loss: nan\n",
      "Epoch: 130/200... Step: 390... Loss: 0.8819... Val Loss: nan\n",
      "Epoch: 134/200... Step: 400... Loss: 0.8581... Val Loss: nan\n",
      "Epoch: 137/200... Step: 410... Loss: 0.7757... Val Loss: nan\n",
      "Epoch: 140/200... Step: 420... Loss: 0.7587... Val Loss: nan\n",
      "Epoch: 144/200... Step: 430... Loss: 0.7559... Val Loss: nan\n",
      "Epoch: 147/200... Step: 440... Loss: 0.6746... Val Loss: nan\n",
      "Epoch: 150/200... Step: 450... Loss: 0.6442... Val Loss: nan\n",
      "Epoch: 154/200... Step: 460... Loss: 0.6577... Val Loss: nan\n",
      "Epoch: 157/200... Step: 470... Loss: 0.5961... Val Loss: nan\n",
      "Epoch: 160/200... Step: 480... Loss: 0.5729... Val Loss: nan\n",
      "Epoch: 164/200... Step: 490... Loss: 0.5599... Val Loss: nan\n",
      "Epoch: 167/200... Step: 500... Loss: 0.4917... Val Loss: nan\n",
      "Epoch: 170/200... Step: 510... Loss: 0.4714... Val Loss: nan\n",
      "Epoch: 174/200... Step: 520... Loss: 0.4694... Val Loss: nan\n",
      "Epoch: 177/200... Step: 530... Loss: 0.4184... Val Loss: nan\n",
      "Epoch: 180/200... Step: 540... Loss: 0.4023... Val Loss: nan\n",
      "Epoch: 184/200... Step: 550... Loss: 0.4120... Val Loss: nan\n",
      "Epoch: 187/200... Step: 560... Loss: 0.3849... Val Loss: nan\n",
      "Epoch: 190/200... Step: 570... Loss: 0.3624... Val Loss: nan\n",
      "Epoch: 194/200... Step: 580... Loss: 0.3804... Val Loss: nan\n",
      "Epoch: 197/200... Step: 590... Loss: 0.3179... Val Loss: nan\n",
      "Epoch: 200/200... Step: 600... Loss: 0.2944... Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 200\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4f20f",
   "metadata": {},
   "source": [
    "The training happened over 200 epochs, through which the training loss decreses to reach .29, unfortunately I cannot see the testing loss (I've tried to solve the issue but couldn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "8a4ae28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_200_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              \n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455957f8",
   "metadata": {},
   "source": [
    "Then I saved the latest model with all its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "69ff7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        #x = one_hot_encode(x, len(net.chars))\n",
    "        #x = nn.Embedding(x, embedding_dim)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch[0], p=p/p.sum())\n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141762d",
   "metadata": {},
   "source": [
    "The predict function takes in a character and predicts the next character by randomly picking through the topk characters i.e the characters with the highest probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "05718c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fa1a1",
   "metadata": {},
   "source": [
    "Then the sample function starts off with a prime word, takes as input a character and produces the next one to be also used as input in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "1615597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellosed apous an is desserong it while could not and own that the ories at oft it as upites.  The Count whom I was to protuch which condorded shincis ould his upon upon ahe and ofler. “This which were is a presoning where I way a ginot ous handed sever uncelf on in on tried, and I soul the coll and it on you will the dieds of I sare that she would call advery of betile of had and sutting. I am sure to see his hand. “Werlowed the sidelisabenst of his houpens to see misher and house, and had extull the mas of such. A blist de at the leatter facts of selpect in orely to mery in the ochear. A shay be one to lett hemped tale and a briver und oncilinst.”  “I tollsed you care ut ah of Iich I abt of he save time seven the drove of on him and that of a sab-afmal tame a subple thaw I could not have shatged in the chair.  “It is cobpers. I call to to have the dack into that she would be a post tandies, of you crust in a dire of his sidelight promade and his armes of a cry of “Fire!” The cood perays it\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Hello', top_k=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967447e",
   "metadata": {},
   "source": [
    "Finally I tried to generate some text, the results are not very good seen as the text generated doesn't make much sense but the network did get some words right still."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
